= Website Technology Fingerprinting CLI
:toc:
:toclevels: 3

== 1. System Description (App Purpose)

=== Role
A passive technology-fingerprinting engine that analyzes websites to identify 150+ underlying technologies including frontend frameworks, backend platforms, infrastructure, analytics tools, CMSs, CDNs, security components, fonts, icons, and hosting providers.

=== Core Objective
Given one or more URLs, the system inspects:

* HTTP headers
* HTML structure and patterns
* JavaScript variables, libraries, and inline scripts
* CSS frameworks and patterns
* Network resources (fonts, images, videos, icons)
* DNS and TLS metadata
* Cookies and forms
* Favicon hashes
* SRI (Subresource Integrity) hashes
* HTML/JS/CSS comments
* Asset URLs (CDNs, hosting platforms)

It returns a structured, explainable list of detected technologies with confidence scores.

=== Expected Capabilities
* Stateless execution per target
* Deterministic output for identical inputs
* Modular detection rules (10 YAML files)
* 19 pluggable analyzers (headers, HTML, JS, CSS, DNS, TLS, cookies, forms, SRI, favicon, comments, assets, etc.)
* CLI-first, automation-friendly design
* Async/concurrent I/O operations
* Confidence aggregation with evidence provenance

=== Output Contract
Each detection must include:

* `name`
* `category`
* `version` (if inferred)
* `confidence`
* `evidence` (type, value, pattern)


== 2. High-Level Architecture

[source]
----
main.py                  # CLI entry point

core/
 ├── engine.py           # Orchestration, 19 analyzers
 ├── context.py          # Immutable ScanContext
 └── pipeline.py         # Analyzer pipeline

fetch/
 ├── http_client.py      # HTTP fetching (httpx)
 ├── dns_client.py       # DNS resolution (dnspython)
 └── tls_client.py       # TLS certificate info

analyzers/
 ├── headers.py          # Standard (6)
 ├── html.py
 ├── meta_tags.py
 ├── js.py
 ├── css.py
 ├── http_details.py
 ├── network.py          # Extended (7)
 ├── cookies.py
 ├── robots_sitemap.py
 ├── endpoints.py
 ├── structured_data.py
 ├── pwa.py
 ├── tls_client.py
 ├── script_content.py   # Phase 1 Passive (5)
 ├── favicon.py
 ├── forms.py
 ├── sri.py
 ├── comments.py
 └── assets.py           # Assets (1)

rules/
 ├── backend.yaml        # 33 technologies
 ├── frontend.yaml       # 35 technologies
 ├── javascript.yaml
 ├── css.yaml            # 27 frameworks
 ├── cookies.yaml        # 29 platforms
 ├── network.yaml
 ├── favicon.yaml        # 5 CMS
 ├── forms.yaml          # 10 frameworks
 ├── sri.yaml            # 10 libraries
 ├── assets.yaml         # 30+ rules
 └── rules_loader.py

models/
 ├── detection.py
 └── technology.py

tests/
 ├── test_engine.py
 ├── test_new_analyzers.py
 ├── test_passive_analyzers.py
 └── test_assets_analyzer.py
----

== 3. Core Design Principles

=== 3.1 Single Responsibility Principle
Each module must do exactly one thing.

Examples:
* `http_client.py` handles HTTP fetching only
* `headers.py` analyzes HTTP headers only
* `rules/*.yaml` define detection logic only
* `engine.py` orchestrates execution only

Never mix:
* Fetching and detection
* Parsing and formatting
* Rule definition and execution

---

=== 3.2 Loose Coupling
Modules communicate via shared data contracts.

.Bad
[source,python]
----
headers.detect(response, html_parser)
----

.Good
[source,python]
----
headers.detect(context)
----

---

=== 3.3 Open–Closed Principle
Adding a new technology must not require modifying source code.

Detection rules must be:
* Data-driven
* Declarative
* Externalized (YAML / JSON)

.Rule example
[source,yaml]
----
name: React
category: frontend
evidence:
  - type: js_global
    pattern: "__REACT_DEVTOOLS_GLOBAL_HOOK__"
----

---

=== 3.4 Deterministic Execution
* Identical input produces identical output
* No randomness
* Explicit analyzer order
* Stable confidence calculations

---

== 4. Detection Strategy

=== Detection Types (Hits)
Multiple weak signals are preferred over a single strong signal.

Supported hit types (27+):
* header
* html_pattern
* meta_tag
* js_url
* js_global
* css_url
* css_class
* cookie_name
* cookie_domain
* dns_txt
* dns_mx
* dns_ns
* tls_issuer
* tls_subject
* http_version
* server_timing_entry
* script_src_pattern
* favicon_hash
* form_action_pattern
* hidden_field_name
* sri_hash
* script_content_pattern
* inline_js_variable
* font_src_pattern
* image_src_pattern
* video_embed_pattern

Each hit contributes to a confidence score.

---

=== Confidence Scoring
* Each hit has a weighted value
* Total confidence is capped at `1.0`
* Detection is valid only above a threshold
* Formula: C_total = min(1.0, Σ confidence_i)

[source,python]
----
confidence = min(1.0, sum(hit.weights))
----

Avoid detection based on a single weak indicator.

---

== 5. Scan Context Object

All analyzers operate on a shared, immutable context.

[source,python]
----
@dataclass(frozen=True)
class ScanContext:
    url: str
    status_code: int | None
    headers: dict[str, str]
    body: str
    html: BeautifulSoup | None
    scripts: list[str]              # <script src="...">
    stylesheets: list[str]          # <link rel="stylesheet" href="...">
    meta_tags: dict[str, str]
    cookies: dict[str, dict]
    dns_records: dict[str, list]    # A, MX, TXT, NS
    tls_info: dict | None
    json_ld: list[dict]
    pwa_manifest: dict | None
    robots_txt: str | None
    inline_scripts: list[str]       # <script>...</script> content
    forms: list[dict]               # Action, method, input fields
    comments: list[str]             # HTML/JS/CSS comments
    favicon_hash: str | None        # MD5 hash of favicon
    sri_hashes: list[dict]          # {algorithm, hash, url}
    asset_urls: dict[str, list]     # fonts, images, videos, icons
----

Benefits:
* No side effects
* Immutable after creation (frozen=True)
* Pure analyzer functions
* Comprehensive data from multiple sources
* Explicit dependencies
* High testability

---

== 6. Modularity Rules

* An analyzer must:
** Accept a context object
** Return detections
** Never fetch data
** Never produce output

* The engine must:
** Orchestrate analyzers
** Merge detections
** Resolve conflicts
** Apply confidence thresholds

* Output modules must:
** Format results only
** Never modify detection data

---

== 7. CLI Best Practices

* Input sources:
** Single URL
** File input
** Standard input (pipe)

* Exit codes:
** `0` Success
** `1` Network failure
** `2` Invalid input

* Common flags:
** `--json`
** `--confidence-threshold`
** `--categories frontend,backend`

---

== 8. Testing Strategy

* Unit tests per analyzer
* Golden-file tests for known targets
* Fully mocked network layer
* Detection rule schema validation

---

== 9. Performance Guidelines

* One HTTP request per URL by default
* Asynchronous fetch layer
* Skip analyzers when context data is missing
* Cache DNS and TLS lookups

---

== 12. Open to any Ideas
---
== 13. Version control (git)
  * Any new feature, new branch, test then merge on success 
---
== 13. Inspire from wappalyzer

[quote]
____
Act as a passive website technology fingerprinting engine.  
Analyze URLs using modular, data-driven detection rules.  
Produce deterministic, explainable, structured output.  
Enforce single responsibility, loose coupling, and open-closed design principles.
____

